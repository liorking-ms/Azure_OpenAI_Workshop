{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_COMPLETION_DEPLOYMENT_NAME\")\n",
    "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "OPENAI_DEPLOYMENT_VERSION = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = OPENAI_DEPLOYMENT_ENDPOINT, \n",
    "  api_key=OPENAI_API_KEY,  \n",
    "  api_version=OPENAI_DEPLOYMENT_VERSION\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm(model=OPENAI_MODEL_NAME,\n",
    "             deployment_name=OPENAI_DEPLOYMENT_NAME,\n",
    "             openai_api_version=OPENAI_DEPLOYMENT_VERSION,\n",
    "             azure_endpoint = OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "             temperature=0,\n",
    "             max_tokens=400,\n",
    "             top_p=1,\n",
    "             ):\n",
    "\n",
    "    llm = AzureOpenAI(deployment_name=deployment_name,\n",
    "                      model=model,\n",
    "                      openai_api_version=openai_api_version,\n",
    "                      temperature=temperature,\n",
    "                      azure_endpoint = azure_endpoint,\n",
    "                      max_tokens=max_tokens,\n",
    "                      top_p=top_p,\n",
    "                      model_kwargs={\"stop\": [\"<|im_end|>\"]}\n",
    "                      )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Use Langchain Prompt Templates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Assessing the risk tolerance of a new client is an important step in developing a successful financial trading strategy. To do this, we typically use a risk assessment questionnaire that asks the client about their financial goals, investment experience, and risk preferences. This helps us understand their risk appetite and determine the level of risk they are comfortable with. We also take into consideration their financial situation and investment objectives to ensure that their risk tolerance aligns with their overall financial plan. It's important to regularly reassess a client's risk tolerance as it may change over time."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = init_llm()\n",
    "# create template for prompt using embedded variables\n",
    "template = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "            \n",
    "            <|im_end|>\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"profession\", \"expertise\", \"question\"])\n",
    "answer = llm(prompt_template.format(profession=\"Financial Trading Consultant\",  \n",
    "                                    expertise=\"Risk Management\",\n",
    "                                    question=\"How do you assess the risk tolerance of a new client?\"))\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "It's not clear or the question is not related to Risk Management."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# asking unrelated question\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"profession\", \"expertise\", \"question\"])\n",
    "answer = llm(prompt_template.format(profession=\"Financial Trading Consultant\",  \n",
    "                                    expertise=\"Risk Management\",\n",
    "                                    question=\"What's the fastest car in the world? Answer in one sentence. \"))\n",
    "display(Markdown(answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
